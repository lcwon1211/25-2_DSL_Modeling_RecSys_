# -*- coding: utf-8 -*-
"""cold.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BpWk6fl2GjBi6--iG7QBF9zn-kcT9Ph3
"""

import pandas as pd
import numpy as np
import os
import time
from tqdm import tqdm
from collections import defaultdict

# PyTorch 및 PyG 라이브러리가 필요합니다.
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_scatter import scatter_softmax

# --------------------------------------------------------------------------
# 0단계: 설정값 (Hyperparameters)
# --------------------------------------------------------------------------
BATCH_SIZE = 1024
LEARNING_RATE = 0.001
EMBEDDING_DIM = 32
EPOCHS = 30
TOP_K_FOR_EVAL = [20, 100]
PATIENCE = 5
# ******** 핵심 수정 사항 시작 ********
CHECKPOINT_PATH = 'best_model_cold_start.pth' # Cold Start 모델을 위한 체크포인트
# ******** 핵심 수정 사항 종료 ********

# --------------------------------------------------------------------------
# 1단계: 데이터 로드 및 전처리 클래스
# --------------------------------------------------------------------------
class KGDataPyG:
    """PyTorch Geometric(PyG)에 최적화된 형태로 데이터를 로드하고 전처리합니다."""
    def __init__(self, data_dir='.', train_file='train_interactions_cold.csv', val_file='validation_interactions_cold.csv'):
        print("1. PyG용 데이터 로드 및 전처리를 시작합니다...")
        try:
            # ******** 핵심 수정 사항 시작 ********
            # Cold Start용 데이터 파일을 불러옵니다.
            kg_df = pd.read_csv(os.path.join(data_dir, 'kg_triples_final.csv'))
            self.train_df = pd.read_csv(os.path.join(data_dir, train_file))
            self.validation_df = pd.read_csv(os.path.join(data_dir, val_file))
            # ******** 핵심 수정 사항 종료 ********
        except FileNotFoundError as e:
            raise FileNotFoundError(f"필수 파일({e.filename})을 찾을 수 없습니다. Cold Start 데이터 생성 스크립트를 먼저 실행하세요.")

        entities = pd.concat([kg_df['subject'], kg_df['object']]).unique()
        self.entity2id = {e: i for i, e in enumerate(entities)}
        self.n_entities = len(entities)
        relations = kg_df['predicate'].unique()
        self.relation2id = {r: i for i, r in enumerate(relations)}
        self.n_relations = len(relations)
        item_ids_str = kg_df[kg_df['subject'].str.startswith('Route_')]['subject'].unique()
        self.all_item_ids = [self.entity2id[i] for i in item_ids_str if i in self.entity2id]

        src = [self.entity2id.get(s) for s in kg_df['subject']]
        dest = [self.entity2id.get(o) for o in kg_df['object']]
        rel = [self.relation2id.get(p) for p in kg_df['predicate']]

        self.edge_index = torch.tensor([src + dest, dest + src])
        self.edge_type = torch.tensor(rel + rel)

        print(f"   - 전처리 완료! 총 개체: {self.n_entities}, 총 관계: {self.n_relations}")

class InteractionDataset(Dataset):
    def __init__(self, df, entity2id):
        self.users = df['user_id'].apply(lambda x: entity2id.get(x, -1)).values
        self.items = df['route_id'].apply(lambda x: entity2id.get(x, -1)).values
        self.labels = df['interaction'].values
    def __len__(self): return len(self.labels)
    def __getitem__(self, idx): return self.users[idx], self.items[idx], self.labels[idx]

# --------------------------------------------------------------------------
# 2단계: PyG 기반 KGAT 모델 정의 (이전과 동일)
# --------------------------------------------------------------------------
class KGATLayerPyG(MessagePassing):
    def __init__(self, in_dim, out_dim):
        super().__init__(aggr='add')
        self.W = nn.Linear(in_dim * 2, 1)
        self.leaky_relu = nn.LeakyReLU(0.2)
    def forward(self, x, edge_index, edge_type, relation_embed):
        edge_attr = relation_embed[edge_type]
        return self.propagate(edge_index, x=x, edge_attr=edge_attr)
    def message(self, x_i, x_j, edge_attr, index, ptr, size_i):
        tail_relation_embed = x_j * edge_attr
        attn_input = torch.cat([x_i, tail_relation_embed], dim=1)
        alpha = self.leaky_relu(self.W(attn_input))
        alpha = scatter_softmax(alpha, index, dim=0)
        return alpha * x_j

class KGATPyG(nn.Module):
    def __init__(self, n_entities, n_relations, embed_dim):
        super(KGATPyG, self).__init__()
        self.entity_embedding = nn.Embedding(n_entities, embed_dim)
        self.relation_embedding = nn.Embedding(n_relations, embed_dim)
        self.gat_layer = KGATLayerPyG(embed_dim, embed_dim)
        nn.init.xavier_uniform_(self.entity_embedding.weight)
        nn.init.xavier_uniform_(self.relation_embedding.weight)
    def get_final_embeddings(self, edge_index, edge_type):
        aggregated_embeds = self.gat_layer(self.entity_embedding.weight, edge_index, edge_type, self.relation_embedding.weight)
        return self.entity_embedding.weight + aggregated_embeds
    def forward(self, user_ids, item_ids, edge_index, edge_type):
        final_entity_embeds = self.get_final_embeddings(edge_index, edge_type)
        user_embeds = final_entity_embeds[user_ids]
        item_embeds = final_entity_embeds[item_ids]
        scores = torch.sum(user_embeds * item_embeds, dim=1)
        return scores

# --------------------------------------------------------------------------
# 3단계: 훈련 및 검증 함수 (이전과 동일)
# --------------------------------------------------------------------------
def train_one_epoch(model, dataloader, optimizer, criterion, device, edge_index, edge_type):
    model.train(); total_loss = 0
    for users, items, labels in tqdm(dataloader, desc="Training", leave=False):
        if (users == -1).any() or (items == -1).any(): continue
        users, items, labels = users.to(device), items.to(device), labels.to(device)
        optimizer.zero_grad()
        scores = model(users, items, edge_index, edge_type)
        loss = criterion(scores, labels.float()); loss.backward(); optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def evaluate_model(model, validation_df, data, device, k_list=TOP_K_FOR_EVAL):
    model.eval(); results = {f'Recall@{k}': [] for k in k_list}; results.update({f'NDCG@{k}': [] for k in k_list}); results.update({f'HitRate@{k}': [] for k in k_list})
    with torch.no_grad(): final_entity_embeds = model.get_final_embeddings(data.edge_index.to(device), data.edge_type.to(device))
    all_item_ids_tensor = torch.tensor(data.all_item_ids, device=device)
    validation_pos_df = validation_df[validation_df['interaction'] == 1]; max_k = max(k_list)
    for user_id_str, group in tqdm(validation_pos_df.groupby('user_id'), desc="Evaluating", leave=False):
        if user_id_str not in data.entity2id: continue
        user_id_tensor = torch.tensor([data.entity2id[user_id_str]], device=device)
        ground_truth_items = set(group['route_id'].apply(lambda x: data.entity2id.get(x)))
        if not ground_truth_items: continue
        with torch.no_grad():
            user_embed = final_entity_embeds[user_id_tensor]; all_item_embeds = final_entity_embeds[all_item_ids_tensor]
            all_scores = torch.sum(user_embed * all_item_embeds, dim=1); _, top_k_indices = torch.topk(all_scores, max_k)
            top_max_k_items = all_item_ids_tensor[top_k_indices].tolist()
            for k in k_list:
                top_k_items = top_max_k_items[:k]; hits = len(ground_truth_items.intersection(set(top_k_items)))
                results[f'Recall@{k}'].append(hits / len(ground_truth_items)); results[f'HitRate@{k}'].append(1.0 if hits > 0 else 0.0)
                relevance = [1 if item in ground_truth_items else 0 for item in top_k_items]
                ideal_relevance = [1] * min(k, len(ground_truth_items))
                dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance)]); idcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevance)])
                ndcg = dcg / idcg if idcg > 0 else 0; results[f'NDCG@{k}'].append(ndcg)
    final_results = {key: np.mean(val) for key, val in results.items()}
    return final_results

# --------------------------------------------------------------------------
# 4단계: 메인 실행
# --------------------------------------------------------------------------
if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}\n")
    try:
        # Cold Start용 데이터 파일을 불러옵니다.
        kg_data = KGDataPyG(train_file='train_interactions_cold.csv', val_file='validation_interactions_cold.csv')
    except FileNotFoundError as e:
        print(e); exit()

    print("\n2. Cold Start 모델 훈련 및 검증을 시작합니다...")
    train_dataloader = DataLoader(InteractionDataset(kg_data.train_df, kg_data.entity2id), batch_size=BATCH_SIZE, shuffle=True)
    model = KGATPyG(kg_data.n_entities, kg_data.n_relations, EMBEDDING_DIM).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCEWithLogitsLoss()

    edge_index, edge_type = kg_data.edge_index.to(device), kg_data.edge_type.to(device)

    best_recall = 0.0; patience_counter = 0;
    if os.path.exists(CHECKPOINT_PATH):
        print(f"'{CHECKPOINT_PATH}'에서 저장된 모델을 불러옵니다..."); model.load_state_dict(torch.load(CHECKPOINT_PATH))

    for epoch in range(1, EPOCHS + 1):
        avg_loss = train_one_epoch(model, train_dataloader, optimizer, criterion, device, edge_index, edge_type)
        eval_scores = evaluate_model(model, kg_data.validation_df, kg_data, device)
        current_recall = eval_scores.get(f'Recall@{TOP_K_FOR_EVAL[0]}', 0)
        print(f"Epoch {epoch:02d}/{EPOCHS} | Train Loss: {avg_loss:.4f} | Validation Recall@{TOP_K_FOR_EVAL[0]}: {current_recall:.4f}")
        if current_recall > best_recall:
            print(f"   -> ✅ Validation Recall improved. Saving best model to '{CHECKPOINT_PATH}'...")
            best_recall = current_recall; torch.save(model.state_dict(), CHECKPOINT_PATH); patience_counter = 0
        else:
            patience_counter += 1; print(f"   -> ⚠️ Validation Recall did not improve. Patience: {patience_counter}/{PATIENCE}")
        if patience_counter >= PATIENCE:
            print(f"\n✋ Early stopping triggered."); break

    print("\n3. 최종 모델 성능을 평가합니다...")
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"오류: '{CHECKPOINT_PATH}' 파일이 생성되지 않았습니다."); exit()
    best_model = KGATPyG(kg_data.n_entities, kg_data.n_relations, EMBEDDING_DIM).to(device)
    best_model.load_state_dict(torch.load(CHECKPOINT_PATH))
    final_scores = evaluate_model(best_model, kg_data.validation_df, kg_data, device)
    print("\n" + "="*50); print("      최종 모델 성능 평가 결과 (Cold Start)"); print("="*50)
    for k in TOP_K_FOR_EVAL:
        print(f"Recall@{k} : {final_scores.get(f'Recall@{k}', 0):.4f}")
        print(f"NDCG@{k}   : {final_scores.get(f'NDCG@{k}', 0):.4f}")
        print(f"HitRate@{k}: {final_scores.get(f'HitRate@{k}', 0):.4f}")
        if k != TOP_K_FOR_EVAL[-1]: print("-" * 20)
    print("="*50)

import pandas as pd
import numpy as np
import os
import time
from tqdm import tqdm
from collections import defaultdict

# PyTorch 및 PyG 라이브러리가 필요합니다.
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_scatter import scatter_softmax

# --------------------------------------------------------------------------
# 0단계: 설정값 (Hyperparameters) - 개선됨
# --------------------------------------------------------------------------
BATCH_SIZE = 1024
# ******** 성능 개선 수정 사항 ********
LEARNING_RATE = 0.0005 # 학습률을 더 작게 설정하여 안정적인 수렴 유도
EMBEDDING_DIM = 64     # 임베딩 차원을 64로 늘려 모델의 표현력 강화
# ******** 성능 개선 수정 사항 종료 ********
EPOCHS = 30
TOP_K_FOR_EVAL = [20, 100]
PATIENCE = 5
CHECKPOINT_PATH = 'best_model_cold_start_v2.pth' # 개선된 모델을 위한 새 체크포인트

# --------------------------------------------------------------------------
# 1단계: 데이터 로드 및 전처리 클래스
# --------------------------------------------------------------------------
class KGDataPyG:
    """PyTorch Geometric(PyG)에 최적화된 형태로 데이터를 로드하고 전처리합니다."""
    def __init__(self, data_dir='.', train_file='train_interactions_cold.csv', val_file='validation_interactions_cold.csv'):
        print("1. PyG용 데이터 로드 및 전처리를 시작합니다...")
        try:
            kg_df = pd.read_csv(os.path.join(data_dir, 'kg_triples_final.csv'))
            self.train_df = pd.read_csv(os.path.join(data_dir, train_file))
            self.validation_df = pd.read_csv(os.path.join(data_dir, val_file))
        except FileNotFoundError as e:
            raise FileNotFoundError(f"필수 파일({e.filename})을 찾을 수 없습니다. Cold Start 데이터 생성 스크립트를 먼저 실행하세요.")

        entities = pd.concat([kg_df['subject'], kg_df['object']]).unique()
        self.entity2id = {e: i for i, e in enumerate(entities)}
        self.n_entities = len(entities)
        relations = kg_df['predicate'].unique()
        self.relation2id = {r: i for i, r in enumerate(relations)}
        self.n_relations = len(relations)
        item_ids_str = kg_df[kg_df['subject'].str.startswith('Route_')]['subject'].unique()
        self.all_item_ids = [self.entity2id[i] for i in item_ids_str if i in self.entity2id]

        src = [self.entity2id.get(s) for s in kg_df['subject']]
        dest = [self.entity2id.get(o) for o in kg_df['object']]
        rel = [self.relation2id.get(p) for p in kg_df['predicate']]

        self.edge_index = torch.tensor([src + dest, dest + src])
        self.edge_type = torch.tensor(rel + rel)

        print(f"   - 전처리 완료! 총 개체: {self.n_entities}, 총 관계: {self.n_relations}")

class InteractionDataset(Dataset):
    def __init__(self, df, entity2id):
        self.users = df['user_id'].apply(lambda x: entity2id.get(x, -1)).values
        self.items = df['route_id'].apply(lambda x: entity2id.get(x, -1)).values
        self.labels = df['interaction'].values
    def __len__(self): return len(self.labels)
    def __getitem__(self, idx): return self.users[idx], self.items[idx], self.labels[idx]

# --------------------------------------------------------------------------
# 2단계: PyG 기반 KGAT 모델 정의 (개선됨)
# --------------------------------------------------------------------------
class KGATLayerPyG(MessagePassing):
    """PyG의 MessagePassing을 상속받아 효율적으로 구현한 어텐션 레이어"""
    def __init__(self, in_dim, out_dim):
        super().__init__(aggr='add')
        self.W = nn.Linear(in_dim * 2, 1)
        self.leaky_relu = nn.LeakyReLU(0.2)
    def forward(self, x, edge_index, edge_type, relation_embed):
        edge_attr = relation_embed[edge_type]
        return self.propagate(edge_index, x=x, edge_attr=edge_attr)
    def message(self, x_i, x_j, edge_attr, index, ptr, size_i):
        tail_relation_embed = x_j * edge_attr
        attn_input = torch.cat([x_i, tail_relation_embed], dim=1)
        alpha = self.leaky_relu(self.W(attn_input))
        alpha = scatter_softmax(alpha, index, dim=0)
        return alpha * x_j

class KGATPyG(nn.Module):
    """
    ***** 성능 개선 수정 사항 *****
    - 2개의 GAT 레이어를 사용하여 더 깊은 관계(2-hop 이웃)를 학습합니다.
    """
    def __init__(self, n_entities, n_relations, embed_dim):
        super(KGATPyG, self).__init__()
        self.entity_embedding = nn.Embedding(n_entities, embed_dim)
        self.relation_embedding = nn.Embedding(n_relations, embed_dim)

        self.gat_layer1 = KGATLayerPyG(embed_dim, embed_dim)
        self.gat_layer2 = KGATLayerPyG(embed_dim, embed_dim) # 두 번째 레이어 추가

        nn.init.xavier_uniform_(self.entity_embedding.weight)
        nn.init.xavier_uniform_(self.relation_embedding.weight)

    def get_final_embeddings(self, edge_index, edge_type):
        x1 = self.gat_layer1(self.entity_embedding.weight, edge_index, edge_type, self.relation_embedding.weight)
        h1 = F.relu(self.entity_embedding.weight + x1)
        x2 = self.gat_layer2(h1, edge_index, edge_type, self.relation_embedding.weight)
        h2 = h1 + x2
        return h2

    def forward(self, user_ids, item_ids, edge_index, edge_type):
        final_entity_embeds = self.get_final_embeddings(edge_index, edge_type)
        user_embeds = final_entity_embeds[user_ids]
        item_embeds = final_entity_embeds[item_ids]
        scores = torch.sum(user_embeds * item_embeds, dim=1)
        return scores

# --------------------------------------------------------------------------
# 3단계: 훈련 및 검증 함수
# --------------------------------------------------------------------------
def train_one_epoch(model, dataloader, optimizer, criterion, device, edge_index, edge_type):
    model.train(); total_loss = 0
    for users, items, labels in tqdm(dataloader, desc="Training", leave=False):
        if (users == -1).any() or (items == -1).any(): continue
        users, items, labels = users.to(device), items.to(device), labels.to(device)
        optimizer.zero_grad()
        scores = model(users, items, edge_index, edge_type)
        loss = criterion(scores, labels.float()); loss.backward(); optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def evaluate_model(model, validation_df, data, device, k_list=TOP_K_FOR_EVAL):
    model.eval(); results = {f'Recall@{k}': [] for k in k_list}; results.update({f'NDCG@{k}': [] for k in k_list}); results.update({f'HitRate@{k}': [] for k in k_list})
    with torch.no_grad(): final_entity_embeds = model.get_final_embeddings(data.edge_index.to(device), data.edge_type.to(device))
    all_item_ids_tensor = torch.tensor(data.all_item_ids, device=device)
    validation_pos_df = validation_df[validation_df['interaction'] == 1]; max_k = max(k_list)
    for user_id_str, group in tqdm(validation_pos_df.groupby('user_id'), desc="Evaluating", leave=False):
        if user_id_str not in data.entity2id: continue
        user_id_tensor = torch.tensor([data.entity2id[user_id_str]], device=device)
        ground_truth_items = set(group['route_id'].apply(lambda x: data.entity2id.get(x)))
        if not ground_truth_items: continue
        with torch.no_grad():
            user_embed = final_entity_embeds[user_id_tensor]; all_item_embeds = final_entity_embeds[all_item_ids_tensor]
            all_scores = torch.sum(user_embed * all_item_embeds, dim=1); _, top_k_indices = torch.topk(all_scores, max_k)
            top_max_k_items = all_item_ids_tensor[top_k_indices].tolist()
            for k in k_list:
                top_k_items = top_max_k_items[:k]; hits = len(ground_truth_items.intersection(set(top_k_items)))
                results[f'Recall@{k}'].append(hits / len(ground_truth_items)); results[f'HitRate@{k}'].append(1.0 if hits > 0 else 0.0)
                relevance = [1 if item in ground_truth_items else 0 for item in top_k_items]
                ideal_relevance = [1] * min(k, len(ground_truth_items))
                dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance)]); idcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevance)])
                ndcg = dcg / idcg if idcg > 0 else 0; results[f'NDCG@{k}'].append(ndcg)
    final_results = {key: np.mean(val) for key, val in results.items()}
    return final_results

# --------------------------------------------------------------------------
# 4단계: 메인 실행
# --------------------------------------------------------------------------
if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}\n")
    try:
        kg_data = KGDataPyG(train_file='train_interactions_cold.csv', val_file='validation_interactions_cold.csv')
    except FileNotFoundError as e:
        print(e); exit()

    print("\n2. Cold Start 모델 훈련 및 검증을 시작합니다 (v2)...")
    train_dataloader = DataLoader(InteractionDataset(kg_data.train_df, kg_data.entity2id), batch_size=BATCH_SIZE, shuffle=True)
    model = KGATPyG(kg_data.n_entities, kg_data.n_relations, EMBEDDING_DIM).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCEWithLogitsLoss()

    edge_index, edge_type = kg_data.edge_index.to(device), kg_data.edge_type.to(device)

    best_recall = 0.0; patience_counter = 0;
    if os.path.exists(CHECKPOINT_PATH):
        print(f"'{CHECKPOINT_PATH}'에서 저장된 모델을 불러옵니다..."); model.load_state_dict(torch.load(CHECKPOINT_PATH))

    for epoch in range(1, EPOCHS + 1):
        avg_loss = train_one_epoch(model, train_dataloader, optimizer, criterion, device, edge_index, edge_type)
        eval_scores = evaluate_model(model, kg_data.validation_df, kg_data, device)
        current_recall = eval_scores.get(f'Recall@{TOP_K_FOR_EVAL[0]}', 0)
        print(f"Epoch {epoch:02d}/{EPOCHS} | Train Loss: {avg_loss:.4f} | Validation Recall@{TOP_K_FOR_EVAL[0]}: {current_recall:.4f}")
        if current_recall > best_recall:
            print(f"   -> ✅ Validation Recall improved. Saving best model to '{CHECKPOINT_PATH}'...")
            best_recall = current_recall; torch.save(model.state_dict(), CHECKPOINT_PATH); patience_counter = 0
        else:
            patience_counter += 1; print(f"   -> ⚠️ Validation Recall did not improve. Patience: {patience_counter}/{PATIENCE}")
        if patience_counter >= PATIENCE:
            print(f"\n✋ Early stopping triggered."); break

    print("\n3. 최종 모델 성능을 평가합니다...")
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"오류: '{CHECKPOINT_PATH}' 파일이 생성되지 않았습니다."); exit()
    best_model = KGATPyG(kg_data.n_entities, kg_data.n_relations, EMBEDDING_DIM).to(device)
    best_model.load_state_dict(torch.load(CHECKPOINT_PATH))
    final_scores = evaluate_model(best_model, kg_data.validation_df, kg_data, device)
    print("\n" + "="*50); print("      최종 모델 성능 평가 결과 (Cold Start v2)"); print("="*50)
    for k in TOP_K_FOR_EVAL:
        print(f"Recall@{k} : {final_scores.get(f'Recall@{k}', 0):.4f}")
        print(f"NDCG@{k}   : {final_scores.get(f'NDCG@{k}', 0):.4f}")
        print(f"HitRate@{k}: {final_scores.get(f'HitRate@{k}', 0):.4f}")
        if k != TOP_K_FOR_EVAL[-1]: print("-" * 20)
    print("="*50)


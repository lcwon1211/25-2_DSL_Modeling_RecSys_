# -*- coding: utf-8 -*-
"""traintest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VQewh3-G4jkQd8meeD5nKNA5TkJBnlxz
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import os
from tqdm import tqdm

# --------------------------------------------------------------------------
# 설정값
# --------------------------------------------------------------------------
# 입력 파일: 사용자별 전체 경로 순위가 담긴 파일
INPUT_RANKINGS_PATH = 'user_preferred_route.csv'

# ******** 핵심 수정 사항 시작 ********
# 출력 파일 경로 (Warm Start용으로 명확하게 구분)
TRAIN_OUTPUT_PATH = 'train_interactions_warm.csv'
VALIDATION_OUTPUT_PATH = 'validation_interactions_warm.csv'
# ******** 핵심 수정 사항 종료 ********

# 분할 비율 (검증용 경로 그룹으로 20%를 사용)
VALIDATION_SET_SIZE = 0.2

# 상호작용 데이터 생성 시 사용할 샘플링 개수
TOP_K_POSITIVE = 1000
N_NEGATIVE_TRAIN_SAMPLES = 3200

# --------------------------------------------------------------------------
# 메인 실행 로직
# --------------------------------------------------------------------------
if __name__ == '__main__':
    print("Warm Start용 훈련/검증 데이터 생성을 시작합니다 (경로 기반 분할).\n")

    # --- 1. 원본 랭킹 데이터 로드 및 변환 ---
    try:
        user_rankings = pd.read_csv(INPUT_RANKINGS_PATH)
        print(f"'{INPUT_RANKINGS_PATH}' 파일을 성공적으로 불러왔습니다.")
    except FileNotFoundError as e:
        print(f"오류: 파일을 찾을 수 없습니다. 경로를 확인하세요. -> {e}")
        exit()

    rankings_long = user_rankings.melt(
        id_vars=['user_id'],
        var_name='rank_str',
        value_name='route_id'
    ).dropna()
    rankings_long['route_id'] = rankings_long['route_id'].astype(int)

    # --- 2. 경로(코스) 그룹 분할 ---
    print(f"\n1. 전체 경로를 {1-VALIDATION_SET_SIZE:.0%}:{VALIDATION_SET_SIZE:.0%} 비율로 분할합니다...")

    unique_routes = rankings_long['route_id'].unique()

    train_route_ids, validation_route_ids = train_test_split(
        unique_routes,
        test_size=VALIDATION_SET_SIZE,
        random_state=42
    )

    train_routes_set = set(train_route_ids)
    validation_routes_set = set(validation_route_ids)

    print(f"   - 훈련용 경로 수: {len(train_routes_set)}")
    print(f"   - 검증용 경로 수: {len(validation_routes_set)}")

    # --- 3. 긍정적 상호작용(정답) 데이터 생성 ---
    print("\n2. 각 사용자의 긍정적 상호작용(Top 1,000개) 데이터를 추출합니다...")

    positive_interactions = rankings_long[
        pd.to_numeric(rankings_long['rank_str'].str.replace('rank_', '')) <= TOP_K_POSITIVE
    ]

    # --- 4. 훈련용/검증용 데이터셋 생성 ---
    print("\n3. 훈련용 및 검증용 데이터셋을 최종 생성합니다...")

    # 4-1. 검증용 데이터셋
    validation_df = positive_interactions[
        positive_interactions['route_id'].isin(validation_routes_set)
    ][['user_id', 'route_id']]
    validation_df['interaction'] = 1
    validation_df['user_id'] = "User_" + validation_df['user_id'].astype(str)
    validation_df['route_id'] = "Route_" + validation_df['route_id'].astype(str)

    # 4-2. 훈련용 데이터셋
    train_positives = positive_interactions[
        positive_interactions['route_id'].isin(train_routes_set)
    ][['user_id', 'route_id']]
    train_positives['interaction'] = 1

    train_negatives_list = []

    for user_id, group in tqdm(train_positives.groupby('user_id'), desc="   - 훈련용 부정 샘플링 중"):
        user_positive_routes = set(group['route_id'])
        negative_pool = list(train_routes_set - user_positive_routes)
        num_to_sample = min(len(negative_pool), N_NEGATIVE_TRAIN_SAMPLES)
        sampled_negatives = np.random.choice(negative_pool, size=num_to_sample, replace=False)
        for route_id in sampled_negatives:
            train_negatives_list.append([user_id, route_id, 0])

    train_negatives = pd.DataFrame(train_negatives_list, columns=['user_id', 'route_id', 'interaction'])
    train_df = pd.concat([train_positives, train_negatives], ignore_index=True)
    train_df['user_id'] = "User_" + train_df['user_id'].astype(str)
    train_df['route_id'] = "Route_" + train_df['route_id'].astype(str)
    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)

    # --- 5. 최종 파일 저장 ---
    train_df.to_csv(TRAIN_OUTPUT_PATH, index=False)
    validation_df.to_csv(VALIDATION_OUTPUT_PATH, index=False)

    print("\n" + "="*50)
    print(f"      Warm Start 데이터셋 생성 완료!")
    print("="*50)
    print(f"훈련용 데이터셋 ('{TRAIN_OUTPUT_PATH}')")
    print(f"  - 총 데이터 수: {len(train_df)}")
    print(f"검증용 데이터셋 ('{VALIDATION_OUTPUT_PATH}')")
    print(f"  - 총 데이터 수: {len(validation_df)}")
    print("="*50)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import os
from tqdm import tqdm

# --------------------------------------------------------------------------
# 설정값
# --------------------------------------------------------------------------
# 입력 파일: 사용자별 전체 경로 순위가 담긴 파일
INPUT_RANKINGS_PATH = 'user_preferred_route.csv'

# 출력 파일 경로 (Cold Start용으로 명확하게 구분)
TRAIN_OUTPUT_PATH = 'train_interactions_cold.csv'
VALIDATION_OUTPUT_PATH = 'validation_interactions_cold.csv'

# 분할 비율 (검증용 사용자 그룹으로 20%를 사용)
VALIDATION_SET_SIZE = 0.2

# 상호작용 데이터 생성 시 사용할 샘플링 개수
TOP_K_POSITIVE = 1000
BOTTOM_K_NEGATIVE = 2000

# --------------------------------------------------------------------------
# 메인 실행 로직
# --------------------------------------------------------------------------
if __name__ == '__main__':
    print("Cold Start용 훈련/검증 데이터 생성을 시작합니다 (사용자 기반 분할).\n")

    # --- 1. 원본 랭킹 데이터 로드 ---
    try:
        user_rankings = pd.read_csv(INPUT_RANKINGS_PATH)
        print(f"'{INPUT_RANKINGS_PATH}' 파일을 성공적으로 불러왔습니다.")
    except FileNotFoundError as e:
        print(f"오류: 파일을 찾을 수 없습니다. 경로를 확인하세요. -> {e}")
        exit()

    # --- 2. 사용자 그룹 분할 ---
    print(f"\n1. 전체 사용자를 {1-VALIDATION_SET_SIZE:.0%}:{VALIDATION_SET_SIZE:.0%} 비율로 분할합니다...")

    unique_users = user_rankings['user_id'].unique()

    train_user_ids, validation_user_ids = train_test_split(
        unique_users,
        test_size=VALIDATION_SET_SIZE,
        random_state=42
    )

    print(f"   - 훈련용 사용자 수: {len(train_user_ids)}")
    print(f"   - 검증용 사용자 수: {len(validation_user_ids)}")

    # --- 3. 상호작용 데이터 생성 ---
    print("\n2. 각 사용자 그룹에 대해 상호작용 데이터를 생성합니다...")

    rankings_long = user_rankings.melt(
        id_vars=['user_id'],
        var_name='rank_str',
        value_name='route_id'
    ).dropna()
    rankings_long['rank'] = pd.to_numeric(rankings_long['rank_str'].str.replace('rank_', ''))

    total_ranks = rankings_long['rank'].max()

    train_interactions = []
    validation_interactions = []

    # 훈련용 사용자 그룹에 대해 데이터 생성
    train_rankings = rankings_long[rankings_long['user_id'].isin(train_user_ids)]
    for user_id, group in tqdm(train_rankings.groupby('user_id'), desc="   - 훈련용 데이터 생성 중"):
        user_id_str = f"User_{int(user_id)}"
        pos = group[group['rank'] <= TOP_K_POSITIVE]
        neg = group[group['rank'] > total_ranks - BOTTOM_K_NEGATIVE]
        for _, row in pos.iterrows():
            train_interactions.append([user_id_str, f"Route_{int(row['route_id'])}", 1])
        for _, row in neg.iterrows():
            train_interactions.append([user_id_str, f"Route_{int(row['route_id'])}", 0])

    # 검증용 사용자 그룹에 대해 데이터 생성
    validation_rankings = rankings_long[rankings_long['user_id'].isin(validation_user_ids)]
    for user_id, group in tqdm(validation_rankings.groupby('user_id'), desc="   - 검증용 데이터 생성 중"):
        user_id_str = f"User_{int(user_id)}"
        pos = group[group['rank'] <= TOP_K_POSITIVE]
        neg = group[group['rank'] > total_ranks - BOTTOM_K_NEGATIVE]
        for _, row in pos.iterrows():
            validation_interactions.append([user_id_str, f"Route_{int(row['route_id'])}", 1])
        for _, row in neg.iterrows():
            validation_interactions.append([user_id_str, f"Route_{int(row['route_id'])}", 0])

    train_df = pd.DataFrame(train_interactions, columns=['user_id', 'route_id', 'interaction'])
    validation_df = pd.DataFrame(validation_interactions, columns=['user_id', 'route_id', 'interaction'])

    print("   - 상호작용 데이터 생성 완료.")

    # --- 4. 최종 파일 저장 ---
    train_df.to_csv(TRAIN_OUTPUT_PATH, index=False)
    validation_df.to_csv(VALIDATION_OUTPUT_PATH, index=False)

    print("\n" + "="*50)
    print(f"      Cold Start 데이터셋 생성 완료!")
    print("="*50)
    print(f"훈련용 데이터셋 ('{TRAIN_OUTPUT_PATH}')")
    print(f"  - 총 데이터 수: {len(train_df)}")
    print(f"검증용 데이터셋 ('{VALIDATION_OUTPUT_PATH}')")
    print(f"  - 총 데이터 수: {len(validation_df)}")
    print("="*50)

import pandas as pd
import numpy as np
import os
import time
from tqdm import tqdm
from collections import defaultdict

# PyTorch 및 PyG 라이브러리가 필요합니다.
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_scatter import scatter_softmax

# --------------------------------------------------------------------------
# 0단계: 설정값 (Hyperparameters)
# --------------------------------------------------------------------------
BATCH_SIZE = 1024
LEARNING_RATE = 0.001
EMBEDDING_DIM = 32
EPOCHS = 30
TOP_K_FOR_EVAL = [1]
PATIENCE = 5
# ******** 핵심 수정 사항 시작 ********
CHECKPOINT_PATH = 'best_model_cold_start.pth' # Cold Start 모델을 위한 체크포인트
# ******** 핵심 수정 사항 종료 ********

# --------------------------------------------------------------------------
# 1단계: 데이터 로드 및 전처리 클래스
# --------------------------------------------------------------------------
class KGDataPyG:
    """PyTorch Geometric(PyG)에 최적화된 형태로 데이터를 로드하고 전처리합니다."""
    def __init__(self, data_dir='.', train_file='train_interactions_cold.csv', val_file='validation_interactions_cold.csv'):
        print("1. PyG용 데이터 로드 및 전처리를 시작합니다...")
        try:
            # ******** 핵심 수정 사항 시작 ********
            # Cold Start용 데이터 파일을 불러옵니다.
            kg_df = pd.read_csv(os.path.join(data_dir, 'kg_triples_final.csv'))
            self.train_df = pd.read_csv(os.path.join(data_dir, train_file))
            self.validation_df = pd.read_csv(os.path.join(data_dir, val_file))
            # ******** 핵심 수정 사항 종료 ********
        except FileNotFoundError as e:
            raise FileNotFoundError(f"필수 파일({e.filename})을 찾을 수 없습니다. Cold Start 데이터 생성 스크립트를 먼저 실행하세요.")

        entities = pd.concat([kg_df['subject'], kg_df['object']]).unique()
        self.entity2id = {e: i for i, e in enumerate(entities)}
        self.n_entities = len(entities)
        relations = kg_df['predicate'].unique()
        self.relation2id = {r: i for i, r in enumerate(relations)}
        self.n_relations = len(relations)
        item_ids_str = kg_df[kg_df['subject'].str.startswith('Route_')]['subject'].unique()
        self.all_item_ids = [self.entity2id[i] for i in item_ids_str if i in self.entity2id]

        src = [self.entity2id.get(s) for s in kg_df['subject']]
        dest = [self.entity2id.get(o) for o in kg_df['object']]
        rel = [self.relation2id.get(p) for p in kg_df['predicate']]

        self.edge_index = torch.tensor([src + dest, dest + src])
        self.edge_type = torch.tensor(rel + rel)

        print(f"   - 전처리 완료! 총 개체: {self.n_entities}, 총 관계: {self.n_relations}")

class InteractionDataset(Dataset):
    def __init__(self, df, entity2id):
        self.users = df['user_id'].apply(lambda x: entity2id.get(x, -1)).values
        self.items = df['route_id'].apply(lambda x: entity2id.get(x, -1)).values
        self.labels = df['interaction'].values
    def __len__(self): return len(self.labels)
    def __getitem__(self, idx): return self.users[idx], self.items[idx], self.labels[idx]

# --------------------------------------------------------------------------
# 2단계: PyG 기반 KGAT 모델 정의 (이전과 동일)
# --------------------------------------------------------------------------
class KGATLayerPyG(MessagePassing):
    def __init__(self, in_dim, out_dim):
        super().__init__(aggr='add')
        self.W = nn.Linear(in_dim * 2, 1)
        self.leaky_relu = nn.LeakyReLU(0.2)
    def forward(self, x, edge_index, edge_type, relation_embed):
        edge_attr = relation_embed[edge_type]
        return self.propagate(edge_index, x=x, edge_attr=edge_attr)
    def message(self, x_i, x_j, edge_attr, index, ptr, size_i):
        tail_relation_embed = x_j * edge_attr
        attn_input = torch.cat([x_i, tail_relation_embed], dim=1)
        alpha = self.leaky_relu(self.W(attn_input))
        alpha = scatter_softmax(alpha, index, dim=0)
        return alpha * x_j

class KGATPyG(nn.Module):
    def __init__(self, n_entities, n_relations, embed_dim):
        super(KGATPyG, self).__init__()
        self.entity_embedding = nn.Embedding(n_entities, embed_dim)
        self.relation_embedding = nn.Embedding(n_relations, embed_dim)
        self.gat_layer = KGATLayerPyG(embed_dim, embed_dim)
        nn.init.xavier_uniform_(self.entity_embedding.weight)
        nn.init.xavier_uniform_(self.relation_embedding.weight)
    def get_final_embeddings(self, edge_index, edge_type):
        aggregated_embeds = self.gat_layer(self.entity_embedding.weight, edge_index, edge_type, self.relation_embedding.weight)
        return self.entity_embedding.weight + aggregated_embeds
    def forward(self, user_ids, item_ids, edge_index, edge_type):
        final_entity_embeds = self.get_final_embeddings(edge_index, edge_type)
        user_embeds = final_entity_embeds[user_ids]
        item_embeds = final_entity_embeds[item_ids]
        scores = torch.sum(user_embeds * item_embeds, dim=1)
        return scores

# --------------------------------------------------------------------------
# 3단계: 훈련 및 검증 함수 (이전과 동일)
# --------------------------------------------------------------------------
def train_one_epoch(model, dataloader, optimizer, criterion, device, edge_index, edge_type):
    model.train(); total_loss = 0
    for users, items, labels in tqdm(dataloader, desc="Training", leave=False):
        if (users == -1).any() or (items == -1).any(): continue
        users, items, labels = users.to(device), items.to(device), labels.to(device)
        optimizer.zero_grad()
        scores = model(users, items, edge_index, edge_type)
        loss = criterion(scores, labels.float()); loss.backward(); optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def evaluate_model(model, validation_df, data, device, k_list=TOP_K_FOR_EVAL):
    model.eval(); results = {f'Recall@{k}': [] for k in k_list}; results.update({f'NDCG@{k}': [] for k in k_list}); results.update({f'HitRate@{k}': [] for k in k_list})
    with torch.no_grad(): final_entity_embeds = model.get_final_embeddings(data.edge_index.to(device), data.edge_type.to(device))
    all_item_ids_tensor = torch.tensor(data.all_item_ids, device=device)
    validation_pos_df = validation_df[validation_df['interaction'] == 1]; max_k = max(k_list)
    for user_id_str, group in tqdm(validation_pos_df.groupby('user_id'), desc="Evaluating", leave=False):
        if user_id_str not in data.entity2id: continue
        user_id_tensor = torch.tensor([data.entity2id[user_id_str]], device=device)
        ground_truth_items = set(group['route_id'].apply(lambda x: data.entity2id.get(x)))
        if not ground_truth_items: continue
        with torch.no_grad():
            user_embed = final_entity_embeds[user_id_tensor]; all_item_embeds = final_entity_embeds[all_item_ids_tensor]
            all_scores = torch.sum(user_embed * all_item_embeds, dim=1); _, top_k_indices = torch.topk(all_scores, max_k)
            top_max_k_items = all_item_ids_tensor[top_k_indices].tolist()
            for k in k_list:
                top_k_items = top_max_k_items[:k]; hits = len(ground_truth_items.intersection(set(top_k_items)))
                results[f'Recall@{k}'].append(hits / len(ground_truth_items)); results[f'HitRate@{k}'].append(1.0 if hits > 0 else 0.0)
                relevance = [1 if item in ground_truth_items else 0 for item in top_k_items]
                ideal_relevance = [1] * min(k, len(ground_truth_items))
                dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance)]); idcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevance)])
                ndcg = dcg / idcg if idcg > 0 else 0; results[f'NDCG@{k}'].append(ndcg)
    final_results = {key: np.mean(val) for key, val in results.items()}
    return final_results

# --------------------------------------------------------------------------
# 4단계: 메인 실행
# --------------------------------------------------------------------------
if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}\n")
    try:
        # Cold Start용 데이터 파일을 불러옵니다.
        kg_data = KGDataPyG(train_file='train_interactions_cold.csv', val_file='validation_interactions_cold.csv')
    except FileNotFoundError as e:
        print(e); exit()

    print("\n2. Cold Start 모델 훈련 및 검증을 시작합니다...")
    train_dataloader = DataLoader(InteractionDataset(kg_data.train_df, kg_data.entity2id), batch_size=BATCH_SIZE, shuffle=True)
    model = KGATPyG(kg_data.n_entities, kg_data.n_relations, EMBEDDING_DIM).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCEWithLogitsLoss()

    edge_index, edge_type = kg_data.edge_index.to(device), kg_data.edge_type.to(device)

    best_recall = 0.0; patience_counter = 0;
    if os.path.exists(CHECKPOINT_PATH):
        print(f"'{CHECKPOINT_PATH}'에서 저장된 모델을 불러옵니다..."); model.load_state_dict(torch.load(CHECKPOINT_PATH))

    for epoch in range(1, EPOCHS + 1):
        avg_loss = train_one_epoch(model, train_dataloader, optimizer, criterion, device, edge_index, edge_type)
        eval_scores = evaluate_model(model, kg_data.validation_df, kg_data, device)
        current_recall = eval_scores.get(f'Recall@{TOP_K_FOR_EVAL[0]}', 0)
        print(f"Epoch {epoch:02d}/{EPOCHS} | Train Loss: {avg_loss:.4f} | Validation Recall@{TOP_K_FOR_EVAL[0]}: {current_recall:.4f}")
        if current_recall > best_recall:
            print(f"   -> ✅ Validation Recall improved. Saving best model to '{CHECKPOINT_PATH}'...")
            best_recall = current_recall; torch.save(model.state_dict(), CHECKPOINT_PATH); patience_counter = 0
        else:
            patience_counter += 1; print(f"   -> ⚠️ Validation Recall did not improve. Patience: {patience_counter}/{PATIENCE}")
        if patience_counter >= PATIENCE:
            print(f"\n✋ Early stopping triggered."); break

    print("\n3. 최종 모델 성능을 평가합니다...")
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"오류: '{CHECKPOINT_PATH}' 파일이 생성되지 않았습니다."); exit()
    best_model = KGATPyG(kg_data.n_entities, kg_data.n_relations, EMBEDDING_DIM).to(device)
    best_model.load_state_dict(torch.load(CHECKPOINT_PATH))
    final_scores = evaluate_model(best_model, kg_data.validation_df, kg_data, device)
    print("\n" + "="*50); print("      최종 모델 성능 평가 결과 (Cold Start)"); print("="*50)
    for k in TOP_K_FOR_EVAL:
        print(f"Recall@{k} : {final_scores.get(f'Recall@{k}', 0):.4f}")
        print(f"NDCG@{k}   : {final_scores.get(f'NDCG@{k}', 0):.4f}")
        print(f"HitRate@{k}: {final_scores.get(f'HitRate@{k}', 0):.4f}")
        if k != TOP_K_FOR_EVAL[-1]: print("-" * 20)
    print("="*50)